{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJi0PD1yhjbN",
        "outputId": "6d6cff34-14aa-4eb0-926d-a6c580341ff0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWAOWV1_jcCm",
        "outputId": "b0e375e5-e906-4b7a-af1e-9648e1cc875b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.12.2)\n",
            "Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/431.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m317.4/431.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "import nltk\n",
        "import emoji\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "metadata": {
        "id": "OOuQNNzajd6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/gdrive/MyDrive/tweet_data.csv')"
      ],
      "metadata": {
        "id": "K1u6ODGvjq4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ibYCyA-jtO-",
        "outputId": "caf8dc3d-2bae-4d5d-dcdf-90f3f671af84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "tk = TweetTokenizer()"
      ],
      "metadata": {
        "id": "CNQz1lYdjvEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = [\"i\", \"am\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
        "        \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
        "        \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\",\n",
        "        \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\",\n",
        "        \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"if\", \"or\", \"because\", \"as\",\n",
        "        \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"between\", \"into\", \"through\",\n",
        "        \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\",\n",
        "        \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\",\n",
        "        \"any\", \"both\", \"each\", \"other\", \"such\", \"own\", \"same\", \"so\", \"than\", \"s\", \"t\", \"can\", \"will\", \"just\", \"now\"]\n",
        "\n",
        "punctuations = [\"+\", \",\", \".\", \"-\", \"\\\\\", \"&\", \"!\", \"?\", \":\", \";\", \"#\", \"~\", \"=\", \"/\", \"$\", \"Â£\", \"^\", \"(\", \")\", \"_\", \"<\", \">\", \"%\"]"
      ],
      "metadata": {
        "id": "5EBl8-cmjw1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_smileys = {\n",
        "        \":-)\":\"smiley\", \":-]\":\"smiley\", \":-3\":\"smiley\", \":->\":\"smiley\", \"8-)\":\"smiley\", \":-}\":\"smiley\", \":)\":\"smiley\", \":]\":\"smiley\",\n",
        "        \":3\":\"smiley\", \":>\":\"smiley\", \"8)\":\"smiley\", \":}\":\"smiley\", \":o)\":\"smiley\", \":c)\":\"smiley\", \":^)\":\"smiley\", \"=]\":\"smiley\",\n",
        "        \"=)\":\"smiley\", \":-))\":\"smiley\", \":â€‘D\":\"smiley\", \"8â€‘D\":\"smiley\",\"xâ€‘D\":\"smiley\",\"Xâ€‘D\":\"smiley\",\":D\":\"smiley\", \"8D\":\"smiley\",\"xD\":\"smiley\",\n",
        "        \"XD\":\"smiley\", \":â€‘(\":\"sad\", \":â€‘c\":\"sad\",\":â€‘<\":\"sad\",\":â€‘[\":\"sad\",\":(\":\"sad\",\":c\":\"sad\",\":<\":\"sad\",\":[\":\"sad\", \":-||\":\"sad\", \">:[\":\"sad\",\n",
        "        \":{\":\"sad\", \":@\":\"sad\", \">:(\":\"sad\", \":'â€‘(\":\"sad\", \":'(\":\"sad\", \":â€‘P\":\"playful\", \"Xâ€‘P\":\"playful\", \"xâ€‘p\":\"playful\", \":â€‘p\":\"playful\",\n",
        "        \":â€‘Ã\":\"playful\", \":â€‘Ã¾\":\"playful\", \":â€‘b\":\"playful\", \":P\":\"playful\", \"XP\":\"playful\", \"xp\":\"playful\",\":p\":\"playful\",\":Ã\":\"playful\",\n",
        "        \":Ã¾\":\"playful\", \":b\":\"playful\", \"<3\":\"love\"\n",
        "        }\n",
        "\n",
        "\n",
        "dict_contractions = {\n",
        "        \"ain't\":\"is not\", \"amn't\":\"am not\", \"aren't\":\"are not\", \"can't\":\"cannot\", \"'cause\":\"because\", \"couldn't\":\"could not\",\"couldn't've\":\"could not have\",\n",
        "        \"could've\":\"could have\",\"daren't\":\"dare not\",\"daresn't\":\"dare not\",\"dasn't\":\"dare not\",\"didn't\":\"did not\",\"doesn't\":\"does not\",\"don't\":\"do not\",\n",
        "        \"e'er\":\"ever\",\"em\":\"them\",\"everyone's\":\"everyone is\",\"finna\":\"fixing to\",\"gimme\":\"give me\",\"gonna\":\"going to\",\"gon't\":\"go not\",\"gotta\":\"got to\",\n",
        "        \"hadn't\":\"had not\",\"hasn't\":\"has not\",\"haven't\":\"have not\",\"he'd\":\"he would\",\"he'll\":\"he will\",\"he's\":\"he is\",\"he've\":\"he have\",\n",
        "        \"how'd\":\"how would\",\"how'll\":\"how will\", \"how're\":\"how are\", \"how's\":\"how is\", \"I'd\":\"I would\", \"I'll\":\"I will\",\n",
        "        \"I'm\":\"I am\",\"I'm'a\":\"I am about to\",\"I'm'o\":\"I am going to\",\"isn't\":\"is not\",\n",
        "        \"it'd\":\"it would\",\"it'll\":\"it will\",\"it's\":\"it is\",\"I've\":\"I have\",\"kinda\":\"kind of\",\"let's\":\"let us\",\"mayn't\":\"may not\",\n",
        "        \"may've\":\"may have\",\"mightn't\":\"might not\",\"might've\":\"might have\",\"mustn't\":\"must not\", \"mustn't've\":\"must not have\", \"must've\":\"must have\",\n",
        "        \"needn't\":\"need not\",\"ne'er\":\"never\", \"o'\":\"of\", \"o'er\":\"over\", \"ol'\":\"old\", \"oughtn't\":\"ought not\", \"shalln't\":\"shall not\", \"shan't\":\"shall not\",\n",
        "        \"she'd\":\"she would\",\"she'll\":\"she will\",\"she's\":\"she is\",\"shouldn't\":\"should not\",\"shouldn't've\":\"should not have\",\"should've\":\"should have\",\n",
        "        \"somebody's\":\"somebody is\", \"someone's\":\"someone is\", \"something's\":\"something is\", \"that'd\":\"that would\", \"that'll\":\"that will\", \"that're\":\"that are\",\n",
        "        \"that's\":\"that is\",\"there'd\":\"there would\",\"there'll\":\"there will\",\"there're\":\"there are\",\"there's\":\"there is\",\"these're\":\"these are\",\n",
        "        \"they'd\":\"they would\",\"they'll\":\"they will\",\"they're\":\"they are\",\"they've\":\"they have\",\"this's\":\"this is\",\"those're\":\"those are\", \"'tis\":\"it is\",\n",
        "        \"'twas\":\"it was\", \"wanna\":\"want to\", \"wasn't\":\"was not\", \"we'd\":\"we would\", \"we'd've\":\"we would have\", \"we'll\":\"we will\", \"we're\":\"we are\",\n",
        "        \"weren't\":\"were not\",\n",
        "        \"we've\":\"we have\",\"what'd\":\"what did\",\"what'll\":\"what will\",\"what're\":\"what are\",\"what's\":\"what is\",\"what've\":\"what have\",\"when's\":\"when is\",\n",
        "        \"where'd\":\"where did\",\"where're\":\"where are\",\"where's\":\"where is\",\"where've\":\"where have\",\"which's\":\"which is\",\"who'd\":\"who would\",\n",
        "        \"who'd've\":\"who would have\",\"who'll\":\"who will\",\"who're\":\"who are\",\"who's\":\"who is\",\"who've\":\"who have\",\"why'd\":\"why did\",\"why're\":\"why are\",\n",
        "        \"why's\":\"why is\",\"won't\":\"will not\",\"wouldn't\":\"would not\",\"would've\":\"would have\",\"y'all\":\"you all\",\"you'd\":\"you would\",\n",
        "        \"you'll\":\"you will\", \"you're\":\"you are\",\"you've\":\"you have\",\"Whatcha\":\"What are you\",\"luv\":\"love\",\"sux\":\"sucks\"\n",
        "        }"
      ],
      "metadata": {
        "id": "TlqMQNwXjyaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tweet_preprocess(tweet):\n",
        "    tweet = emoji.demojize(tweet)\n",
        "    tweet = tweet.lower()   #Converting to Lowercase\n",
        "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)  #Removing URLs\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)   #Removing re-tweet text\n",
        "    tweet = re.sub(r'\\b\\w\\b', '', tweet) #Removing Single Letters\n",
        "    tweet = re.sub(r'[\\@\\#]', '', tweet)  #Removing Hastags and targets (@)\n",
        "    tweet = re.sub(r'\\d+', '', tweet)  #Removing Numbers\n",
        "    tweet = re.sub(r'\\s+', ' ', tweet).strip()  # Removing extra spaces\n",
        "    for smiley, meaning in dict_smileys.items(): #Emoticons\n",
        "        tweet = tweet.replace(smiley, meaning)\n",
        "    for contraction, expanded in dict_contractions.items():  #Contractions\n",
        "        tweet = tweet.replace(contraction, expanded)\n",
        "    tweet = tk.tokenize(tweet)  #Tokenizing the tweet\n",
        "    pos_tags = pos_tag(tweet)\n",
        "    tweet = [word for word in tweet if word not in stopwords and word not in punctuations]  #Removing Stop words and Punctuations\n",
        "    final_tweet=[]\n",
        "    for token,tag in pos_tag(tweet):   #Lemmatizing the tweet\n",
        "        if token.isdigit():\n",
        "            continue\n",
        "\n",
        "        pos=tag[0].lower()\n",
        "        if pos not in ['a', 'r', 'n', 'v']:\n",
        "            pos='n'\n",
        "        final_tweet.append(lemmatizer.lemmatize(token,pos))\n",
        "\n",
        "    return ' '.join(final_tweet)"
      ],
      "metadata": {
        "id": "waWNY8Ctjz9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string = '#INDEXATION #Budget2024 The removal of indexation and reduction of LTCG to 12.50% is indeed beneficial to everyone ğŸ™‚ .. or say 90% of the people. Below is the example https://t.co/VRqKNvbc2w'\n",
        "string = tweet_preprocess(string)\n",
        "print(string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbGL62Ef9dP1",
        "outputId": "5aead871-1326-4601-cd3f-e0a76f511d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "indexation budget rthemoval indexation reduction ltcg % indeed beneficial everyone slightly_smiling_face .. say % people example\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['processed_text'] = data['Text'].apply(tweet_preprocess)"
      ],
      "metadata": {
        "id": "VHFgNpkIkhhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(max_features=1000)\n",
        "X = vectorizer.fit_transform(data['processed_text']).toarray()\n",
        "terms = vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "pc9md5fUkRDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NMF:\n",
        "    def __init__(self, n_components, max_iter=200, tol=1e-4):\n",
        "        self.n_components = n_components\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize W and H matrices with random non-negative values\n",
        "        np.random.seed(42)\n",
        "        self.W = np.random.rand(n_samples, self.n_components)\n",
        "        self.H = np.random.rand(self.n_components, n_features)\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            # Update H\n",
        "            numerator = np.dot(self.W.T, X)\n",
        "            denominator = np.dot(np.dot(self.W.T, self.W), self.H)\n",
        "            self.H *= numerator / (denominator + 1e-10)\n",
        "\n",
        "            # Update W\n",
        "            numerator = np.dot(X, self.H.T)\n",
        "            denominator = np.dot(np.dot(self.W, self.H), self.H.T)\n",
        "            self.W *= numerator / (denominator + 1e-10)\n",
        "\n",
        "            # Check for convergence\n",
        "            error = np.linalg.norm(X - np.dot(self.W, self.H))\n",
        "            if error < self.tol:\n",
        "                break\n",
        "\n",
        "        return self.W, self.H"
      ],
      "metadata": {
        "id": "QPP6xP6plZx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_topics = 6\n",
        "nmf = NMF(n_components=n_topics)\n",
        "W, H = nmf.fit_transform(X)"
      ],
      "metadata": {
        "id": "oxA6-w5dlgK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_top_terms(H, terms, n_top_terms=6):\n",
        "    for topic_idx, topic in enumerate(H):\n",
        "        top_term_indices = topic.argsort()[-n_top_terms:][::-1]\n",
        "        top_terms = [terms[i] for i in top_term_indices]\n",
        "        print(f\"Topic {topic_idx + 1}: {', '.join(top_terms)}\")"
      ],
      "metadata": {
        "id": "BHO_F1gtli9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_top_terms(H, terms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdE8MDV5lmFD",
        "outputId": "eb2acef9-d1cb-4818-d57e-c54749c553b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: class, middle, budget, nsitharaman, narendramodi, not\n",
            "Topic 2: tax, pay, income, budget, like, more\n",
            "Topic 3: union, budgetforviksitbharat, via, namo, app, budget\n",
            "Topic 4: youth, union, themployment, unionbudget, budget, job\n",
            "Topic 5: india, unionbudget, crore, growth, post, lakh\n",
            "Topic 6: budget, union, finance, not, say, rahulgandhi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_topics = np.argmax(W, axis=1)\n",
        "data['topic'] = tweet_topics"
      ],
      "metadata": {
        "id": "7O350VQ0lplZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    print(f\"Document: {data['Text'].iloc[i][:100]}...\")\n",
        "    print(f\"Topic: {data['topic'].iloc[i] + 1}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBAXAm5NmBuB",
        "outputId": "09fb8e75-29c5-4d44-a28e-71a860d37b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document: Working overtime so I can pay more taxes. Living the dream, one paycheck at a time.\n",
            "#Budget2024...\n",
            "Topic: 2\n",
            "\n",
            "Document: To show fake numbers of jobs having been created, have companies, MNCs been told to float fake vacan...\n",
            "Topic: 5\n",
            "\n",
            "Document: Union Budget 2024: A blueprint for sustainable growth and development\n",
            "https://t.co/QkM6i1qhID via Na...\n",
            "Topic: 3\n",
            "\n",
            "Document: Today he has issues with big businesses!\n",
            "\n",
            "Truth is it's only PM @narendramodi who has taken the Indi...\n",
            "Topic: 6\n",
            "\n",
            "Document: People's leader LOP Shri @RahulGandhi Ji exposed Defence Minister Rajnath Singhâ€™s lies about compens...\n",
            "Topic: 6\n",
            "\n"
          ]
        }
      ]
    }
  ]
}